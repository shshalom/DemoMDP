/**
 * OpenAI Provider Implementation
 *
 * Implements AIProvider interface for OpenAI GPT models.
 */
import OpenAI from 'openai';
import { AIProviderError } from '../contracts/index.js';
/**
 * OpenAI pricing (as of Jan 2025)
 * GPT-4 Turbo: $10.00/M input, $30.00/M output
 * GPT-4: $30.00/M input, $60.00/M output
 * GPT-3.5 Turbo: $0.50/M input, $1.50/M output
 */
const PRICING = {
    'gpt-4-turbo-preview': { input: 10.0 / 1_000_000, output: 30.0 / 1_000_000 },
    'gpt-4-turbo': { input: 10.0 / 1_000_000, output: 30.0 / 1_000_000 },
    'gpt-4': { input: 30.0 / 1_000_000, output: 60.0 / 1_000_000 },
    'gpt-3.5-turbo': { input: 0.5 / 1_000_000, output: 1.5 / 1_000_000 },
    'gpt-4o': { input: 5.0 / 1_000_000, output: 15.0 / 1_000_000 },
    'gpt-4o-mini': { input: 0.15 / 1_000_000, output: 0.6 / 1_000_000 },
};
/**
 * OpenAI embedding pricing
 * text-embedding-3-small: $0.02/M tokens
 * text-embedding-3-large: $0.13/M tokens
 */
const EMBEDDING_PRICING = {
    'text-embedding-3-small': 0.02 / 1_000_000,
    'text-embedding-3-large': 0.13 / 1_000_000,
    'text-embedding-ada-002': 0.1 / 1_000_000,
};
export class OpenAIProvider {
    name = 'openai';
    models = [
        'gpt-4-turbo-preview',
        'gpt-4-turbo',
        'gpt-4',
        'gpt-3.5-turbo',
        'gpt-4o',
        'gpt-4o-mini',
    ];
    client;
    defaultModel;
    defaultEmbeddingModel;
    constructor(options) {
        this.client = new OpenAI({
            apiKey: options.apiKey,
            baseURL: options.baseURL,
            organization: options.organization,
        });
        this.defaultModel = options.defaultModel || 'gpt-4-turbo-preview';
        this.defaultEmbeddingModel = 'text-embedding-3-small';
    }
    async complete(request) {
        const startTime = Date.now();
        const model = request.model || this.defaultModel;
        try {
            // Build messages
            const messages = [
                {
                    role: 'user',
                    content: request.prompt,
                },
            ];
            // Prepare API request
            const params = {
                model,
                messages,
                max_tokens: request.max_tokens,
                temperature: request.temperature ?? 1.0,
                stop: request.stop_sequences,
            };
            // Handle structured output (JSON mode)
            let response;
            let structured;
            if (request.schema) {
                // Use JSON mode with response format
                response = await this.client.chat.completions.create({
                    ...params,
                    response_format: { type: 'json_object' },
                });
                // Parse JSON response
                const content = response.choices[0]?.message?.content || '{}';
                try {
                    structured = JSON.parse(content);
                }
                catch (error) {
                    throw new AIProviderError('Failed to parse JSON response', 'INVALID_RESPONSE', this.name, false);
                }
            }
            else {
                // Standard text completion
                response = await this.client.chat.completions.create(params);
            }
            // Extract content
            const content = response.choices[0]?.message?.content || '';
            // Calculate cost
            const pricing = PRICING[model] || PRICING['gpt-4-turbo-preview'];
            const inputTokens = response.usage?.prompt_tokens || 0;
            const outputTokens = response.usage?.completion_tokens || 0;
            const cost_usd = inputTokens * pricing.input + outputTokens * pricing.output;
            // Map finish reason
            const finishReason = response.choices[0]?.finish_reason;
            const stop_reason = this.mapStopReason(finishReason);
            return {
                content,
                structured,
                stop_reason,
                usage: {
                    input_tokens: inputTokens,
                    output_tokens: outputTokens,
                    total_tokens: inputTokens + outputTokens,
                    cost_usd,
                },
                model: response.model,
                latency_ms: Date.now() - startTime,
            };
        }
        catch (error) {
            if (this.isAPIError(error)) {
                throw this.createProviderError(error);
            }
            throw error;
        }
    }
    async embed(texts) {
        try {
            const response = await this.client.embeddings.create({
                model: this.defaultEmbeddingModel,
                input: texts,
            });
            return response.data.map((item, index) => ({
                text: texts[index],
                vector: item.embedding,
                model: response.model,
                tokens: response.usage?.total_tokens || 0,
            }));
        }
        catch (error) {
            if (this.isAPIError(error)) {
                throw this.createProviderError(error);
            }
            throw error;
        }
    }
    estimateTokens(prompt) {
        // Rough estimation: ~4 characters per token (similar to Claude)
        // This is a simple heuristic; actual tokenization may vary
        const estimatedTokens = Math.ceil(prompt.length / 4);
        const model = this.defaultModel;
        const pricing = PRICING[model] || PRICING['gpt-4-turbo-preview'];
        return {
            tokens: estimatedTokens,
            cost_usd: estimatedTokens * pricing.input,
            model,
        };
    }
    async healthCheck() {
        const startTime = Date.now();
        try {
            // Simple health check: list models
            await this.client.models.list();
            return {
                healthy: true,
                provider: this.name,
                latency_ms: Date.now() - startTime,
            };
        }
        catch (error) {
            return {
                healthy: false,
                provider: this.name,
                latency_ms: Date.now() - startTime,
                error: {
                    code: this.isAPIError(error) ? error.status?.toString() || 'UNKNOWN' : 'UNKNOWN',
                    message: error instanceof Error ? error.message : 'Unknown error',
                },
            };
        }
    }
    mapStopReason(reason) {
        switch (reason) {
            case 'stop':
                return 'complete';
            case 'length':
                return 'length';
            case 'content_filter':
                return 'error';
            case 'function_call':
            case 'tool_calls':
                return 'complete';
            default:
                return 'error';
        }
    }
    isAPIError(error) {
        // Check if error is an APIError by checking its properties and constructor name
        // This handles both real API errors and mocked ones
        if (!error || typeof error !== 'object') {
            return false;
        }
        try {
            // Check if it's an instance of OpenAI.APIError (when SDK is available)
            if (error instanceof OpenAI.APIError) {
                return true;
            }
        }
        catch (e) {
            // instanceof check may fail in test/mock environments
        }
        // Fallback: check if it has the structure of an API error
        return (error instanceof Error &&
            (error.constructor.name === 'APIError' || error.name === 'APIError') &&
            'status' in error);
    }
    createProviderError(error) {
        const code = error.status?.toString() || 'UNKNOWN';
        const recoverable = error.status === 429 || error.status === 503;
        return new AIProviderError(error.message, code, this.name, recoverable);
    }
}
//# sourceMappingURL=openai.js.map