/**
 * Universal Agent Evaluator (Stream 2)
 *
 * AI-powered evaluator that can evaluate ANY policy without custom code.
 * Uses LLM to semantically understand policy rules and identify violations.
 *
 * Key Innovation: Policy authors only write Markdown - no evaluator code needed.
 */
import { PolicyPromptBuilder } from './prompts/prompt-builder.js';
import { ViolationSchema } from './prompts/violation-schema.js';
/**
 * Universal AI-powered policy evaluator
 *
 * Can evaluate any policy by reading its markdown source and using
 * semantic understanding via LLM. No custom evaluator code needed.
 */
export class UniversalAgentEvaluator {
    aiProvider;
    id = 'universal-agent';
    supportedPolicies = ['*']; // Supports all policies
    promptBuilder;
    options;
    constructor(aiProvider, options) {
        this.aiProvider = aiProvider;
        this.promptBuilder = new PolicyPromptBuilder();
        this.options = {
            temperature: options?.temperature ?? 0.1,
            confidence_threshold: options?.confidence_threshold ?? 0.7,
            max_tokens: options?.max_tokens ?? 4096,
            model: options?.model ?? aiProvider.models[0],
        };
    }
    /**
     * Check if this evaluator can handle a policy
     * Universal evaluator can handle any policy
     */
    canEvaluate(_policy) {
        return true;
    }
    /**
     * Evaluate a policy using AI
     */
    async evaluate(context) {
        const startTime = Date.now();
        const { policy } = context;
        try {
            // Check if policy has optimized prompt (new 2-step approach)
            const constraints = policy.constraints;
            const optimizedPrompt = constraints?.violation_check_prompt;
            if (optimizedPrompt) {
                // Use new optimized 2-step evaluation
                return await this.evaluateOptimized(context, optimizedPrompt, startTime);
            }
            // Fall back to full markdown evaluation (backward compatibility)
            return await this.evaluateFull(context, startTime);
        }
        catch (error) {
            // On error, return a failure result
            const errorMessage = error instanceof Error ? error.message : String(error);
            return {
                policy_id: policy.id,
                policy_version: policy.version,
                status: 'fail',
                violations: [
                    {
                        message: `AI evaluation failed: ${errorMessage}`,
                        severity: 'error',
                        metadata: {
                            error: errorMessage,
                            evaluator: 'universal-agent',
                        },
                    },
                ],
                metadata: {
                    duration_ms: Date.now() - startTime,
                },
            };
        }
    }
    /**
     * Optimized 2-step evaluation using pre-built prompts
     *
     * Step 1: Quick yes/no check (~100 tokens)
     * Step 2: Get details only if violation found (~300 tokens)
     */
    async evaluateOptimized(context, violationCheckPrompt, startTime) {
        const { policy } = context;
        // ========================================================================
        // STEP 1: Quick Yes/No Check
        // ========================================================================
        const codeToCheck = this.extractCodeForCheck(context);
        const yesNoPrompt = `${violationCheckPrompt}

Code to check:
\`\`\`
${codeToCheck}
\`\`\`

Answer: yes or no`;
        const yesNoResponse = await this.aiProvider.complete({
            prompt: yesNoPrompt,
            max_tokens: 10, // Just need "yes" or "no"
            temperature: 0,
            model: this.options.model,
        });
        const hasViolation = yesNoResponse.content.toLowerCase().includes('yes');
        // No violation found - return early
        if (!hasViolation) {
            const latency_ms = Date.now() - startTime;
            return {
                policy_id: policy.id,
                policy_version: policy.version,
                status: 'pass',
                violations: [],
                metadata: {
                    model: yesNoResponse.model,
                    tokens_used: yesNoResponse.usage.total_tokens,
                    cost_usd: yesNoResponse.usage.cost_usd,
                    confidence_score: 1.0,
                    latency_ms,
                    fallback_used: false,
                    reasoning: 'No violations detected in quick check',
                },
            };
        }
        // ========================================================================
        // STEP 2: Get Violation Details
        // ========================================================================
        const detailsPrompt = this.buildDetailsPrompt(context);
        const detailsResponse = await this.aiProvider.complete({
            prompt: detailsPrompt,
            schema: ViolationSchema,
            temperature: this.options.temperature,
            max_tokens: this.options.max_tokens,
            model: this.options.model,
        });
        // Parse and filter violations
        const violations = this.parseViolations(detailsResponse.structured);
        const filteredViolations = this.filterByConfidence(violations, this.options.confidence_threshold);
        const status = this.determineStatus(filteredViolations);
        const latency_ms = Date.now() - startTime;
        // Combine token counts and costs from both steps
        const totalTokens = yesNoResponse.usage.total_tokens + detailsResponse.usage.total_tokens;
        const totalCost = yesNoResponse.usage.cost_usd + detailsResponse.usage.cost_usd;
        return {
            policy_id: policy.id,
            policy_version: policy.version,
            status,
            violations: filteredViolations,
            metadata: {
                model: detailsResponse.model,
                tokens_used: totalTokens,
                cost_usd: totalCost,
                confidence_score: this.calculateAverageConfidence(violations),
                latency_ms,
                fallback_used: false,
                reasoning: detailsResponse.structured.overall_assessment,
            },
        };
    }
    /**
     * Full evaluation using complete policy markdown (backward compatibility)
     */
    async evaluateFull(context, startTime) {
        const { policy } = context;
        // 1. Build prompt from context
        const prompt = this.buildPrompt(context);
        // 2. Call LLM with structured output schema
        const response = await this.aiProvider.complete({
            prompt: `${prompt.system}\n\n${prompt.user}`,
            schema: ViolationSchema,
            temperature: this.options.temperature,
            max_tokens: this.options.max_tokens,
            model: this.options.model,
        });
        // 3. Parse violations from structured output
        const violations = this.parseViolations(response.structured);
        // 4. Apply confidence threshold filtering
        const filteredViolations = this.filterByConfidence(violations, this.options.confidence_threshold);
        // 5. Determine overall status
        const status = this.determineStatus(filteredViolations);
        // 6. Calculate metadata
        const latency_ms = Date.now() - startTime;
        const metadata = {
            model: response.model,
            tokens_used: response.usage.total_tokens,
            cost_usd: response.usage.cost_usd,
            confidence_score: this.calculateAverageConfidence(violations),
            latency_ms,
            fallback_used: true, // Indicate we used fallback mode
            reasoning: response.structured.overall_assessment,
        };
        return {
            policy_id: policy.id,
            policy_version: policy.version,
            status,
            violations: filteredViolations,
            metadata,
        };
    }
    /**
     * Estimate cost before evaluation
     */
    async estimateCost(context) {
        // Build prompt to estimate tokens
        const prompt = this.buildPrompt(context);
        const fullPrompt = `${prompt.system}\n\n${prompt.user}`;
        // Estimate tokens using provider
        const estimate = this.aiProvider.estimateTokens(fullPrompt);
        return {
            tokens: estimate.tokens,
            cost_usd: estimate.cost_usd,
            model: estimate.model,
        };
    }
    /**
     * Build prompt based on context type
     */
    buildPrompt(context) {
        if (context.pr) {
            return this.promptBuilder.buildPRPrompt(context);
        }
        else if (context.feature) {
            return this.promptBuilder.buildFeaturePrompt(context);
        }
        else {
            throw new Error('Unsupported evaluation context: requires PR or feature');
        }
    }
    /**
     * Parse violations from structured LLM output
     */
    parseViolations(output) {
        if (!output || !Array.isArray(output.violations)) {
            return [];
        }
        return output.violations.map(v => ({
            message: v.message,
            severity: v.severity,
            location: v.location,
            suggestedFix: v.suggestedFix,
            confidence: v.confidence,
            metadata: {
                reasoning: v.reasoning,
                ai_generated: true,
            },
        }));
    }
    /**
     * Filter violations by confidence threshold
     *
     * Violations below threshold are either:
     * - Converted to warnings (if severity was error)
     * - Kept as-is (if already warning/info)
     */
    filterByConfidence(violations, threshold) {
        return violations
            .map(v => {
            const confidence = v.confidence ?? 1.0;
            if (confidence < threshold) {
                // Downgrade errors to warnings if confidence is low
                if (v.severity === 'error') {
                    return {
                        ...v,
                        severity: 'warning',
                        message: `${v.message} (low confidence: ${confidence.toFixed(2)})`,
                    };
                }
            }
            return v;
        })
            .filter(v => {
            // Filter out very low confidence violations (< 0.5)
            const confidence = v.confidence ?? 1.0;
            return confidence >= 0.5;
        });
    }
    /**
     * Determine overall evaluation status
     */
    determineStatus(violations) {
        if (violations.some(v => v.severity === 'error')) {
            return 'fail';
        }
        if (violations.some(v => v.severity === 'warning')) {
            return 'warning';
        }
        return 'pass';
    }
    /**
     * Calculate average confidence across all violations
     */
    calculateAverageConfidence(violations) {
        if (violations.length === 0) {
            return 1.0; // No violations = perfect confidence
        }
        const sum = violations.reduce((acc, v) => acc + (v.confidence ?? 1.0), 0);
        return sum / violations.length;
    }
    /**
     * Extract code content from context for violation check
     */
    extractCodeForCheck(context) {
        if (context.pr) {
            // For PRs, extract file contents
            const files = context.pr.files.slice(0, 10); // Limit to 10 files for quick check
            return files
                .map(file => {
                const content = file.content || file.patch || '';
                // Truncate each file to 1000 chars for quick check
                const truncated = content.length > 1000
                    ? content.slice(0, 1000) + '\n[... truncated ...]'
                    : content;
                return `// File: ${file.path}\n${truncated}`;
            })
                .join('\n\n');
        }
        else if (context.feature) {
            // For features, extract file contents
            const files = context.feature.files.slice(0, 10);
            return files
                .map(file => {
                const truncated = file.content.length > 1000
                    ? file.content.slice(0, 1000) + '\n[... truncated ...]'
                    : file.content;
                return `// File: ${file.path}\n${truncated}`;
            })
                .join('\n\n');
        }
        return '';
    }
    /**
     * Build detailed prompt for step 2 (get violation details)
     */
    buildDetailsPrompt(context) {
        const { policy } = context;
        const constraints = policy.constraints;
        // Get file type and focus patterns for context
        const fileTypes = constraints?.file_types;
        const focusPatterns = constraints?.focus_patterns;
        let prompt = `This code contains policy violations that need to be identified.

# Policy Information
**ID**: ${policy.id}
**Title**: ${policy.title || 'Untitled Policy'}
**Description**: ${policy.description || ''}
`;
        // Add focus hints if available
        if (focusPatterns && focusPatterns.length > 0) {
            prompt += `\n**Focus Patterns**: ${focusPatterns.join(', ')}\n`;
        }
        // Add code to analyze
        if (context.pr) {
            prompt += `\n# Pull Request Files to Analyze\n\n`;
            const files = context.pr.files.slice(0, 20); // Limit to 20 files
            files.forEach(file => {
                const content = file.content || file.patch || '';
                const truncated = content.length > 10000
                    ? content.slice(0, 10000) + '\n\n[... content truncated ...]'
                    : content;
                prompt += `---\n**File**: ${file.path}\n**Status**: ${file.status}\n**Changes**: +${file.additions} -${file.deletions}\n\n\`\`\`\n${truncated}\n\`\`\`\n\n`;
            });
        }
        else if (context.feature) {
            prompt += `\n# Feature Files to Analyze\n\n`;
            const files = context.feature.files.slice(0, 20);
            files.forEach(file => {
                const truncated = file.content.length > 10000
                    ? file.content.slice(0, 10000) + '\n\n[... content truncated ...]'
                    : file.content;
                prompt += `---\n**File**: ${file.path}\n\n\`\`\`\n${truncated}\n\`\`\`\n\n`;
            });
        }
        prompt += `
# Your Task
Find ALL violations and for each one provide:
1. **Exact location** (file path and line number if possible)
2. **Clear description** of what was violated
3. **Suggested fix** with specific code example
4. **Your confidence** level (0.0-1.0)
5. **Reasoning** explaining why this is a violation

Output as structured JSON matching the schema.
`;
        return prompt;
    }
}
//# sourceMappingURL=universal-agent-evaluator.js.map