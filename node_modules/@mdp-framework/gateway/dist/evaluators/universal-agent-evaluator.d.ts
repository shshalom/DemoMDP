/**
 * Universal Agent Evaluator (Stream 2)
 *
 * AI-powered evaluator that can evaluate ANY policy without custom code.
 * Uses LLM to semantically understand policy rules and identify violations.
 *
 * Key Innovation: Policy authors only write Markdown - no evaluator code needed.
 */
import type { AIProvider } from '@mdp-framework/ai-services';
import type { PolicyCard } from '@mdp-framework/schemas/contracts';
import type { AIEvaluator, EvaluationContext, EvaluationResult, AIEvaluationMetadata } from './types.js';
export interface UniversalAgentEvaluatorOptions {
    /**
     * Temperature for LLM (lower = more deterministic)
     * @default 0.1
     */
    temperature?: number;
    /**
     * Confidence threshold for filtering violations
     * Violations below this threshold are converted to warnings or filtered out
     * @default 0.7
     */
    confidence_threshold?: number;
    /**
     * Maximum tokens for LLM response
     * @default 4096
     */
    max_tokens?: number;
    /**
     * Model to use (if not provider's default)
     */
    model?: string;
}
/**
 * Universal AI-powered policy evaluator
 *
 * Can evaluate any policy by reading its markdown source and using
 * semantic understanding via LLM. No custom evaluator code needed.
 */
export declare class UniversalAgentEvaluator implements AIEvaluator {
    private aiProvider;
    readonly id = "universal-agent";
    readonly supportedPolicies: string[];
    private promptBuilder;
    private options;
    constructor(aiProvider: AIProvider, options?: UniversalAgentEvaluatorOptions);
    /**
     * Check if this evaluator can handle a policy
     * Universal evaluator can handle any policy
     */
    canEvaluate(_policy: PolicyCard): boolean;
    /**
     * Evaluate a policy using AI
     */
    evaluate(context: EvaluationContext): Promise<EvaluationResult & {
        metadata?: AIEvaluationMetadata;
    }>;
    /**
     * Optimized 2-step evaluation using pre-built prompts
     *
     * Step 1: Quick yes/no check (~100 tokens)
     * Step 2: Get details only if violation found (~300 tokens)
     */
    private evaluateOptimized;
    /**
     * Full evaluation using complete policy markdown (backward compatibility)
     */
    private evaluateFull;
    /**
     * Estimate cost before evaluation
     */
    estimateCost(context: EvaluationContext): Promise<{
        tokens: number;
        cost_usd: number;
        model: string;
    }>;
    /**
     * Build prompt based on context type
     */
    private buildPrompt;
    /**
     * Parse violations from structured LLM output
     */
    private parseViolations;
    /**
     * Filter violations by confidence threshold
     *
     * Violations below threshold are either:
     * - Converted to warnings (if severity was error)
     * - Kept as-is (if already warning/info)
     */
    private filterByConfidence;
    /**
     * Determine overall evaluation status
     */
    private determineStatus;
    /**
     * Calculate average confidence across all violations
     */
    private calculateAverageConfidence;
    /**
     * Extract code content from context for violation check
     */
    private extractCodeForCheck;
    /**
     * Build detailed prompt for step 2 (get violation details)
     */
    private buildDetailsPrompt;
}
//# sourceMappingURL=universal-agent-evaluator.d.ts.map